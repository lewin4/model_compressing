dataloader:
  dataset: cifar10
  num_classes: 10
#  imagenet_path: <your_imagenet_path_here>
  root: E:\LY\network-slimming\data.cifar10
  imagenet_path: E:\LY\data\classification_aug
  num_workers: 1
  batch_size: 256
  test_batch_size: 256
  image_shape: [32, 32]
  train_shuffle: True
  validation_shuffle: False

# training hyperparams
epochs: 150
learning_rate: 0.01
momentum: 0.9
weight_decay: 0.0001 # 1e-4

optimizer: adam # must be either sgd or adam

# lr scheduling
lr_scheduler:
  type: cosine
  min_lr: 1.e-7

skip_initial_validation: True
output_path: ../googlenet_cifar10_output/0.8-3multi
use_cuda: True
# model architecture
model:
  arch: googlenet
  model_path: E:\LY\network-slimming\logs\googlenet_cifar10_output\refine\0.8_0.8568\model_best.pth.tar
  resume:
  state_dict_compressed:
  aux_logits: True

  compression_parameters:
    ignored_modules:
      # list of layer names that you do not want to compress.
      # We follow Stock et al. "And the bit goes down: revisiting the quantization of deep neural networks", ICLR 2020
      # and do not compress the first 7x7 convolutional layer, as it represents only 0.1-0.05% of the network weights
      - conv1.conv

    k: 256
    fc_subvector_size: 4 # d_fc
    pw_subvector_size: 4 # d_pw
    # Small or large block compression regime for convolutional layers
    large_subvectors: False
    k_means_type: src # kmeans, kmedians, src, slow_src
    k_means_n_iters: 20

    # Used to overwrite configs
    layer_specs:
      fc:
        k: 2048 # Same as BGD
        k_means_type: src
    n_multi_layer: 3
    multi_layer_specs:
      9:
        k: 256
        k_means_type: src
        k_means_n_iters: 100
      4:
        k: 256
        k_means_type: src
        k_means_n_iters: 100
      fc:
        k: 256
        k_means_type: src
        k_means_n_iters: 100

  use_permutations: False
  sls_iterations: 10_000

