dataloader:
  dataset: cifar100
  num_classes: 100
#  imagenet_path: <your_imagenet_path_here>
  root: E:\LY\network-slimming\data.cifar100
  imagenet_path: E:\LY\data\classification_aug
  num_workers: 2
  batch_size: 64
  test_batch_size: 256
  image_shape: [32, 32]
  train_shuffle: True
  validation_shuffle: False

# training hyperparams
epochs: 230
learning_rate: 0.1
momentum: 0.9
weight_decay: 0.0001 # 1e-4

optimizer: sgd # must be either sgd or adam

# lr scheduling
lr_scheduler:
  type: multistep
  min_lr: 1.e-7
  factor: 0.1
  milestones: [80, 120, 160, 200]

skip_initial_validation: False
output_path: ../vgg_cifar100_output/0.4-2multi
use_cuda: True
# model architecture
model:
  arch: vgg19
  model_path: E:\LY\network-slimming\logs\vgg_cifar100_output\refine\0.4\model_best.pth.tar
  resume:
  state_dict_compressed:

  compression_parameters:
    ignored_modules:
      # list of layer names that you do not want to compress.
      # We follow Stock et al. "And the bit goes down: revisiting the quantization of deep neural networks", ICLR 2020
      # and do not compress the first 7x7 convolutional layer, as it represents only 0.1-0.05% of the network weights
      - feature.0
      - classifier

    k: 256
    fc_subvector_size: 4 # d_fc
    pw_subvector_size: 4 # d_pw
    # Small or large block compression regime for convolutional layers
    large_subvectors: False
    k_means_type: src # kmeans, kmedians, src, slow_src
    k_means_n_iters: 20

    # Used to overwrite configs
    layer_specs:
      fc:
        k: 2048 # Same as BGD
        k_means_type: src
    n_multi_layer: 2
    multi_layer_specs:
      9:
        k: 512
        k_means_type: src
        k_means_n_iters: 100
      4:
        k: 512
        k_means_type: src
        k_means_n_iters: 100

  use_permutations: False
  sls_iterations: 10_000

